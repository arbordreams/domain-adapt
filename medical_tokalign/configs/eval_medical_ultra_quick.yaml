# Ultra-quick evaluation profile for iteration (~15â€“25 min on H100).
# - Uses vLLM backend for high-throughput inference
# - Aggressively reduces dataset sizes
# - Disables tokenizer analysis and perplexity
# Production eval remains eval_medical.yaml; quick profile is eval_medical_quick.yaml.

model_id: Qwen/Qwen2-7B
eval_backend: vllm
precision: bf16
attn_impl: flash_attention_2
compile: true
grad_ckpt: false
allow_tf32: true
matmul_precision: high

random_seed: 17
output_dir: medical_tokalign/runs/medical_eval
seed_everything: true
enable_tokenizer_analysis: false
enable_stats: true

vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.92
  max_model_len: 8192
  max_batch_tokens: 65536
  enforce_eager: false
  trust_remote_code: false

hf:
  per_device_batch_size: 8
  max_model_len: 4096
  ppl_batch_size: 64

datasets:
  pubmedqa:
    enabled: true
    split: test
    limit: 150
  medqa_usmle:
    enabled: true
    split: test
    limit: 150
  medmcqa:
    enabled: true
    split: validation
    limit: 300
  mmlu_medical:
    enabled: true
    split: test
    limit: 200
    subjects: [anatomy, clinical_knowledge, professional_medicine, college_medicine]
  mednli:
    enabled: true
    limit: 200
  ncbi_disease:
    enabled: true
    limit: 200
  bc5cdr:
    enabled: true
    limit: 400
  perplexity_corpus: null

gen:
  max_new_tokens: 96
  temperature: 0.0
  top_p: 1.0
  stop: ["\n", "\r\n"]

alignment:
  prefer_adapted_artifacts: true
  search_dir: medical_tokalign/runs/tokenizer_adapt
  use_adapted_model: true
  use_adapted_tokenizer: true
  compare_mode: base_vs_adapted


