# GH200 (ARM64 + H100 96GB) tuned evaluation config

model_id: Qwen/Qwen2-7B

# Backend selection
# On ARM64, vLLM wheels may be unavailable. Start with HF; switch to vLLM after building from source.
eval_backend: hf          # hf | vllm

# Precision and kernels
precision: bf16           # bf16 recommended for H100
attn_impl: flash_attention_2
compile: true             # torch.compile for speed
grad_ckpt: false          # enable only if you hit OOM
allow_tf32: true
matmul_precision: high

random_seed: 17
output_dir: medical_tokalign/runs/medical_eval
seed_everything: true
enable_tokenizer_analysis: true
enable_stats: true

# vLLM sizing for 96GB H100 (if/when enabled)
vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.95
  max_model_len: 8192
  max_batch_tokens: 131072
  enforce_eager: false
  trust_remote_code: false

# HF backend sizing
hf:
  per_device_batch_size: 48   # tune toward 64 if stable on your instance
  max_model_len: 8192
  ppl_batch_size: 128

# Datasets (same as default)
datasets:
  pubmedqa:
    enabled: true
    split: test
    limit: 0
  medqa_usmle:
    enabled: true
    split: test
    limit: 0
  medmcqa:
    enabled: true
    split: validation
    limit: 0
  mmlu_medical:
    enabled: true
    split: test
    limit: 0
    subjects: [anatomy, clinical_knowledge, professional_medicine, college_medicine]
  mednli:
    enabled: true
  ncbi_disease:
    enabled: true
  bc5cdr:
    enabled: true
  perplexity_corpus: pubmed_rct

# Generation & eval settings
gen:
  max_new_tokens: 128
  temperature: 0.0
  top_p: 1.0
  stop: ["\n", "\r\n"]

# Alignment/artifact discovery
alignment:
  prefer_adapted_artifacts: true
  search_dir: medical_tokalign/runs/tokenizer_adapt
  use_adapted_model: true
  use_adapted_tokenizer: true
  compare_mode: base_vs_adapted


