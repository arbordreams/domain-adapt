# H100â€‘tuned evaluation defaults for one NVIDIA H100 PCIe 80GB

model_id: Qwen/Qwen2-7B
eval_backend: vllm        # vllm | hf | tensorrt_llm (stub)
precision: bf16           # bf16 | fp8
attn_impl: flash_attention_2
compile: true
grad_ckpt: false          # inference path; can be true for warmup
allow_tf32: true
matmul_precision: high

random_seed: 17
output_dir: medical_tokalign/runs/medical_eval
seed_everything: true
enable_tokenizer_analysis: true
enable_stats: true

# vLLM sizing for 80GB H100 (adjust if needed)
vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.92
  max_model_len: 8192
  max_batch_tokens: 65536
  enforce_eager: false
  trust_remote_code: false

# HF fallback generation params
hf:
  per_device_batch_size: 8
  max_model_len: 4096
  ppl_batch_size: 64

# Datasets: robust HF sources with standard splits (no full PubMed/PMC eval)
datasets:
  pubmedqa:
    enabled: true
    split: test
    limit: 0
  medqa_usmle:
    enabled: true
    split: test
    limit: 0
  medmcqa:
    enabled: true
    split: validation
    limit: 0
  mmlu_medical:
    enabled: true
    split: test
    limit: 0
    subjects: [anatomy, clinical_knowledge, professional_medicine, college_medicine]
  mednli:
    enabled: true
  ncbi_disease:
    enabled: true
  bc5cdr:
    enabled: true
  perplexity_corpus: pubmed_rct

# Generation & eval settings
gen:
  max_new_tokens: 128
  temperature: 0.0
  top_p: 1.0
  stop: ["\n", "\r\n"]

# Alignment/artifact discovery
alignment:
  prefer_adapted_artifacts: true
  search_dir: medical_tokalign/runs/tokenizer_adapt
  use_adapted_model: true
  use_adapted_tokenizer: true
  compare_mode: base_vs_adapted  # none | base_vs_adapted


